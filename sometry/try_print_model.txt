BASE_Transformer(
  (resnet): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (relu): ReLU()
  (upsamplex2): Upsample(scale_factor=2.0, mode=nearest)
  (upsamplex4): Upsample(scale_factor=4.0, mode=bilinear)
  (classifier): TwoLayerConv2d(
    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (fpn): FPN_S4(
    (conv1by1_4): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
    (conv1by1_3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
    (conv1by1_2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
    (conv1by1_1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
  )
  (conv_pred): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (sigmoid): Sigmoid()
  (conv_a): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (transformer): Transformer(
    (layers): ModuleList(
      (0): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=32, out_features=1536, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=512, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (1): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=32, out_features=1536, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=512, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (2): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=32, out_features=1536, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=512, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (3): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=32, out_features=1536, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=512, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (4): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=32, out_features=1536, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=512, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (5): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=32, out_features=1536, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=512, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
    )
  )
  (transformer_decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): ModuleList(
        (0): Residual2(
          (fn): PreNorm2(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Cross_Attention(
              (to_q): Linear(in_features=32, out_features=64, bias=False)
              (to_k): Linear(in_features=32, out_features=64, bias=False)
              (to_v): Linear(in_features=32, out_features=64, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (1): ModuleList(
        (0): Residual2(
          (fn): PreNorm2(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Cross_Attention(
              (to_q): Linear(in_features=32, out_features=64, bias=False)
              (to_k): Linear(in_features=32, out_features=64, bias=False)
              (to_v): Linear(in_features=32, out_features=64, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
    )
  )
  (diff): Diff(
    (con1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): ReLU()
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (con2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (sk): SKAttention(
    (convs): ModuleList(
      (0): Sequential(
        (conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU()
      )
      (1): Sequential(
        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU()
      )
      (2): Sequential(
        (conv): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU()
      )
      (3): Sequential(
        (conv): Conv2d(32, 32, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))
        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU()
      )
    )
    (fc): Linear(in_features=32, out_features=32, bias=True)
    (fcs): ModuleList(
      (0): Linear(in_features=32, out_features=32, bias=True)
      (1): Linear(in_features=32, out_features=32, bias=True)
      (2): Linear(in_features=32, out_features=32, bias=True)
      (3): Linear(in_features=32, out_features=32, bias=True)
    )
    (softmax): Softmax(dim=0)
  )
)
DSIFN begin!!!!!!!!!!
DSIFN begin!!!!!!!!!!
BASE_Transformer_S5(
  (resnet): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=512, out_features=1000, bias=True)
  )
  (relu): ReLU()
  (upsamplex2): Upsample(scale_factor=2.0, mode=nearest)
  (upsamplex4): Upsample(scale_factor=4.0, mode=bilinear)
  (classifier): TwoLayerConv2d(
    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
    (3): Conv2d(32, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (fpn2): FPN_S5(
    (conv1by1_5): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
    (conv1by1_4): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
    (conv1by1_3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
    (conv1by1_2): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
    (conv1by1_1): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1))
  )
  (conv_pred): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (sigmoid): Sigmoid()
  (conv_a): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (transformer): Transformer(
    (layers): ModuleList(
      (0): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=32, out_features=1536, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=512, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (1): ModuleList(
        (0): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (to_qkv): Linear(in_features=32, out_features=1536, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=512, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
    )
  )
  (transformer_decoder): TransformerDecoder(
    (layers): ModuleList(
      (0): ModuleList(
        (0): Residual2(
          (fn): PreNorm2(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Cross_Attention(
              (to_q): Linear(in_features=32, out_features=64, bias=False)
              (to_k): Linear(in_features=32, out_features=64, bias=False)
              (to_v): Linear(in_features=32, out_features=64, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (1): ModuleList(
        (0): Residual2(
          (fn): PreNorm2(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Cross_Attention(
              (to_q): Linear(in_features=32, out_features=64, bias=False)
              (to_k): Linear(in_features=32, out_features=64, bias=False)
              (to_v): Linear(in_features=32, out_features=64, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (2): ModuleList(
        (0): Residual2(
          (fn): PreNorm2(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Cross_Attention(
              (to_q): Linear(in_features=32, out_features=64, bias=False)
              (to_k): Linear(in_features=32, out_features=64, bias=False)
              (to_v): Linear(in_features=32, out_features=64, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (3): ModuleList(
        (0): Residual2(
          (fn): PreNorm2(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Cross_Attention(
              (to_q): Linear(in_features=32, out_features=64, bias=False)
              (to_k): Linear(in_features=32, out_features=64, bias=False)
              (to_v): Linear(in_features=32, out_features=64, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (4): ModuleList(
        (0): Residual2(
          (fn): PreNorm2(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Cross_Attention(
              (to_q): Linear(in_features=32, out_features=64, bias=False)
              (to_k): Linear(in_features=32, out_features=64, bias=False)
              (to_v): Linear(in_features=32, out_features=64, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
      (5): ModuleList(
        (0): Residual2(
          (fn): PreNorm2(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): Cross_Attention(
              (to_q): Linear(in_features=32, out_features=64, bias=False)
              (to_k): Linear(in_features=32, out_features=64, bias=False)
              (to_v): Linear(in_features=32, out_features=64, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=32, bias=True)
                (1): Dropout(p=0, inplace=False)
              )
            )
          )
        )
        (1): Residual(
          (fn): PreNorm(
            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=32, out_features=64, bias=True)
                (1): GELU(approximate=none)
                (2): Dropout(p=0, inplace=False)
                (3): Linear(in_features=64, out_features=32, bias=True)
                (4): Dropout(p=0, inplace=False)
              )
            )
          )
        )
      )
    )
  )
  (diff): Diff(
    (con1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (act): ReLU()
    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (con2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (eca): ECAAttention(
    (gap): AdaptiveAvgPool2d(output_size=1)
    (conv): Conv1d(1, 1, kernel_size=(3,), stride=(1,), padding=(1,))
    (sigmoid): Sigmoid()
  )
)
